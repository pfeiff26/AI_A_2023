{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b5028e",
   "metadata": {
    "id": "d1b5028e"
   },
   "source": [
    "# Critically Engaging with AI Ethics\n",
    "\n",
    "In this lab we will be critically engaging with existing datasets that have been used to address ethics in AI. In particular, we will explore the [**Jigsaw Toxic Comment Classification Challenge**](https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge). This challenge brought to light bias in the data that sparked the [Jigsaw Unintended Bias in Toxicity Classification Challenge](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification).\n",
    "\n",
    "In this lab, we will dig into the dataset ourselves to explore the biases. We will further explore other datasets to expand our thinking about bias and fairness in AI in relation to aspects such as demography and equal opportunity as well as performance and group unawareness of the model. We will learn more about that in the tutorial below.\n",
    "\n",
    "# Task 1: README!\n",
    "\n",
    "This week, coding activity will be minimal, if any. However, as always, you will be expected to incorporate your analysis, thoughts and discussions into your notebooks as markdown cells, so I recommend you start up your Jupyter notebook in advance. As always, **remember**:\n",
    "\n",
    "- To ensure you have all the necessary Python libraries/packages for running code you are recommended to use your environment set up on the **Glasgow Anywhere Student Desktop**.\n",
    "- Start anaconda, and launch Jupyter Notebook from within Anaconda**. If you run Jupyter Notebook without going through Anaconda, you might not have access to the packages installed on Anaconda.\n",
    "- If you run Anaconda or Jupyter Notebook on a local lab computer, there is no guarantee that these will work properly, that the packages will be available, or that you will have permission to install the extra packages yourself.\n",
    "- You can set up Anaconda on your own computer with the necessary libraries/packages. Please check how to set up a new environement in Anaconda and review the minimum list of Python libraries/packages, all discussed in Week 4 lab.\n",
    "- We strongly recommend that you save your notebooks in the folder you made in Week 1 exercise, which should have been created in the University of Glasgow One Drive - **do not confuse this with personal and other organisational One Drives**. Saving a copy of your notebooks on the University One Drive ensures that it is backed up (the first principles of digital preservation and information mnagement).\n",
    "- When you are on the Remote desktop, the `University of Glasgow One Drive` should be visible in the home directory of the Jupyter Notebook. Other machines may require additional set up and/or navigation for One Drive to be directly accessible from Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3a8e5",
   "metadata": {
    "id": "2ba3a8e5"
   },
   "source": [
    "# Task 2: Identifying Bias\n",
    "\n",
    "This week we will make use of one of the [Kaggle](https://www.kaggle.com) tutorials and their associated notebooks to learn how to identify different types of bias. Biases can creep in at any stage of the AI task, from data collection methods, how we split/organise the test set, different algorithms, how the results are interpreted and deployed. Some of these topics have been extensively discussed and as a response, Kaggle has developed a course on AI ethics:\n",
    "\n",
    "- Navigate to the [Kaggle tutorial on Identifying Bias in AI](https://www.kaggle.com/code/alexisbcook/identifying-bias-in-ai/tutorial).\n",
    "- In this section we will explore the [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge) to discover different types of biases that might emerge in the dataset.\n",
    "\n",
    "#### Task 2-a: Understanding the Scope of Bias\n",
    "\n",
    "Read through the first page of the [Kaggle tutorial on Identifying Bias in AI] to understand the scope of biases discussed at Kaggle.\n",
    "- How many types of biases are described on the page?\n",
    "- Which type of bias did you know about already before this course and which type was new to you?\n",
    "- Can you think of any others? Create a markdown cell below to discuss your thoughts on these questions.\n",
    "\n",
    "Note that the biases discussed in the tutorial are not an exhaustive list. Recall that biases can exist across the entire machine learning pipeline.\n",
    "\n",
    "- Scroll down to the end of the Kaggle tutorial page and click on the link to the exercise to work directly with a model and explore the data.**\n",
    "\n",
    "#### Task 2-b: Run through the tutorial. Take selected screenshorts of your activity while doing the tutorial.\n",
    "\n",
    "- Discuss with your peer group, your findings about the biases in the data, including types of biases.\n",
    "- Demonstrate your discussion with examples and screenshots of your activity on the tutorial. Present these in your own notebook.\n",
    "\n",
    "Modify the markdown cell below to address the Tasks 2-a and 2-b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15d2f9",
   "metadata": {
    "id": "bf15d2f9"
   },
   "source": [
    "## Task 2 - Identifying Bias (Answers)\n",
    "\n",
    "### 2-a\n",
    "- The tutorial describes six different types of bias that can occur in any Machine Learning application\n",
    "\n",
    "- Before reading the Kaggle tutorial, I was aware of historical bias, representation bias and deployment bias. It was eye-opening to learn at how many other stages of the machine learning workflow bias can creep into your application. I was most surprised to learn about evaluation bias, as this was a part of the machine learning process that appeared less vulnerable to bias to me, since it was not directly influencing the output of a model. While I was not aware of their role in a machine learning context, I was somewhat familiar with measurement and aggregation bias in a social science context, as they are similiar to validity and representativeness considerations that inform the research design.\n",
    "\n",
    "- Looking at the visual representation of bias in machine learning in the Kaggle, I notice that there is no bias recognised surrouding the training/test split stage. In light of the MNIST example explored in an earlier Lab task, it is evident that manually choosing the training/test split can be a major source of bias. By using exclusive precisely drawn numbers from adult handwriting as the training data and the messy handwriting of high school students as the test dataset, the model trained on the original NIST dataset achieved lower performance on the student data. Therefore, a dataset might be equally representing groups (lacking representation bias) but if all data points referring to one group are removed from the training data and put into the test dataset, the model will likely exhibit lowered performance on that group.\n",
    "\n",
    "### 2-b\n",
    "\n",
    "![Screenshot of sample toxic and non-toxic comments.](screen-1.jpg)\n",
    "\n",
    "Screenshot of the first part of the tutorial exercise showing examples of toxic and non-toxic comments from the dataset.\n",
    "\n",
    "![Screenshot of \"I fucking love apples.\" comment labelled as toxic.](screen-2.jpg)\n",
    "\n",
    "A limitation of the model that I noticed when feeding the model with my own comments was that it is unable to differentiate between positive and abusive uses of swearwords. The lack of such nuanced understanding highlights the potential to exhibit bias towards words that are often used in a toxic context but not toxic in itself.\n",
    "\n",
    "![Screenshot of \"I have a muslim / black friend\" comment labelled as toxic.](screen-3.jpg)\n",
    "\n",
    "The degree to which this is problematic becomes exemplified when feeding the models with non-toxic comments refering to people's religious or racial identities. The same comments were labeled as non-toxic as soon as \"black\" and \"muslim\" were replaced with \"white\" and \"christian\".\n",
    "\n",
    "The model shows historical bias towards identities that are more frequently subject to online hate, such as black or muslim people. This is because it learns a strong association between the identity descriptors of the people whom the toxic comment is directed to and toxicity itself. \n",
    "\n",
    "Moreover, the tutorial hypothesises how other types of bias could influence the performance of the model. For example, if the training data would include comments that were translated to English from a different language, inaccurate or ambiguous translations could introduce measurement bias. Aggregation bias could also be introduced if comments in different languages were not treated separately. If the model would be deployed to Australia, for example, while being trained and evaluated on comments from UK users, this could also introduce evaluation, deployment and representation bias.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b38fa2",
   "metadata": {
    "id": "45b38fa2"
   },
   "source": [
    "# Task 3: Large Language Models and Bias: Word Embedding Demo\n",
    "\n",
    "Go to the [embedding projector at tensorflow.org](http://projector.tensorflow.org/). This may take some time to load so be patient! There is a lot of information being visualised. This will take especially long if you select \"Word2Vec All\" as your dataset. The projector provides a visualisation of the langauge language model called **Word2Vec**.\n",
    "\n",
    "This tool also provides the option of visualising the organisation of hand written digits from the MNIST dataset to see how data representations of the digits are clustered together or not. There is also the option of visualising the `iris` dataset from `scikit-learn` with respect to their categories. Feel free to explore these as well if you like.\n",
    "\n",
    "For the current exercise, we will concentrate on exploring the relationships between the words in the **Word2Vec** model. First, select **Word2Vec 10K** from the drop down menu (top lefthand side). This is a reduced version of **Word2Vec All**. You can search for words by submitting them in the search box on the right hand side.\n",
    "\n",
    "#### Task 3.1: Initial exploration of words and relationships\n",
    "\n",
    "- Type `apple` and click on `Isolate 101 ppints`. This reduces the noise. Note how juice, fruit, wine are closer together than macintosh, computers and atari.\n",
    "- Try also words like `silver` and `sound`. What are your observations. Does it seem like words related to each other are sitting closer to each other?\n",
    "\n",
    "#### Task 3.2: Exploring \"Word2Vec All\" for patterns\n",
    "\n",
    "- Try to load \"Word2Vec All\" dataset if you can (this may take a while so be patient!) and explore the word `engineer`, `drummer`or any other occupation - what do you find?\n",
    "- Do you think perhaps there are concerns of gender bias? If so, how? If not, why not? Discuss it with our peer group and present the results in a your notebook.\n",
    "- Why not make some screenshots to embed into your notebook along with your comment? This could make it more understandable to a broader audience.\n",
    "- Do not forget to include attribution to the authors of the Projector demo.\n",
    "\n",
    "Modify the markdown cell below to present your thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0317fe1",
   "metadata": {
    "id": "e0317fe1"
   },
   "source": [
    "## Task 3: Bias in Word2Vec Answers\n",
    "\n",
    "### 3.1\n",
    "- Words seem to be closer together the more related to each other they are. It is strange to me that the word apple seems to be far more strongly associated with the electronics brand than the fruit. This is probably due to representation or historical bias within the training data. Since the texts used to train word2vec were probably largely extracted from the internet, the word apple was likely more used in the context of the brand.\n",
    "\n",
    "![screenshot of word2vec results for apple, showing mainly computer-related terms](screen-4.jpg)\n",
    "\n",
    "### 3.2\n",
    "- When exploring the word \"engineer\" two biases become apparent. Firstly, the word engineer is in close proximity to male names, such as spencer, walter, henri and magnus, as well as to the word \"sir\". No female names or the word \"madam\" can be seen anywhere in the space. Furthermore, only European nationalities appear in the space, such as swiss, finnish or belgian. This is due to historial bias, with engineering having been a male-dominated field and most literature likely beeing eurocentric. Although it could also be entirely caused by historical bias, representational bias may play an additional role.\n",
    "\n",
    "![screenshot of word2vec results for engineer](screen-5.jpg)\n",
    "\n",
    "- When exploring the word \"drummer\" a strong gender bias is prevalent. While some female names are visible (kate, kelly) the vast majority of names in proximity to drummer are male. While this is also likely due to historical bias arising from the increased prevalence of male drummers, representational bias may play an additional role.\n",
    "\n",
    "![screenshot of word2vec results for drummer](screen-6.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ffeb50",
   "metadata": {
    "id": "09ffeb50"
   },
   "source": [
    "# Task 4: Thinking about AI Fairness\n",
    "\n",
    "So we now know that AI models (e.g. large language models) can be biased. We saw that with the embedding projector already. We discussed in the previous exercise about the machine learning pipeline, how the assessment of datasets can be crucicial to deciding the suitability of deploying AI in the real world. This is where data connects to questions of fairness.\n",
    "\n",
    "- Navigate to the [Kaggle Tutorial on AI Fairness](https://www.kaggle.com/code/alexisbcook/ai-fairness).\n",
    "\n",
    "#### Task 4-a: Topics in AI Fairness\n",
    "Read through the page to understand the scope of the fairness criteria discussed at Kaggle. Just as we dicussed with bias, the fairness criteria discussed at Kaggle is not exhaustive.\n",
    "- How many criteria are described on the page?\n",
    "- Which criteria did you know about already before this course and which, if any, was new to you?\n",
    "- Can you think of any other criteria? Create a markdown cell and note down your discussion with your peer group on these questions.\n",
    "\n",
    "#### Task 4-b: AI fairness in the context of the credit card dataset.\n",
    "Scroll down to the end of [the page on AI fairness](https://www.kaggle.com/code/alexisbcook/ai-fairness) to find a link to another interactive exercise to run code in a notebook using credit card application data.\n",
    "- Run the tutorial, while taking selected screenshots.\n",
    "- Discuss your findings with your peer group.\n",
    "- Note down the key points of your activity and discussion in your notebook using the example and screenshots of your activity on the tutorial.\n",
    "\n",
    "\n",
    "Report the results of the activity and discussion by modifying the markdown cell below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "493fbafc",
   "metadata": {
    "id": "493fbafc"
   },
   "source": [
    "## Task 4: AI Fairness Answers\n",
    "\n",
    "### 4-a\n",
    "- There are four fairness criteria described on the page, Demographic parity, Equal accuracy, Equal opportunity and Group unaware. While I did not know all the specific names used to describe the criteria, I was aware of the principles of all the criteria before the exploring the Kaggle tutorial. \n",
    "\n",
    "- Another AI fairness approach I can think of is that of \"Affirmative Action\" or \"Positive Action Algorithms\". Some scholars consider the group unaware approach to be an insufficient response and argue that machine learning models should not give every person the same opportunities but actively favour those at a societal disadvantage. They acknowledge that there are legal and ethical limitations to this approach. See e.g. __[Is Algorithmic Affirmative Action Legal? - Georgetown Law](https://www.law.georgetown.edu/georgetown-law-journal/wp-content/uploads/sites/26/2020/04/Is-Algorithmic-Affirmative-Action-Legal.pdf)__\n",
    "\n",
    "### 4-b\n",
    "\n",
    "- The Kaggle tutorial execise works with a dataset containing ficticious credit card applicant information. Attributes included in the dataset are the income of each applicant, the number of children in their household and whether they own a car or housing. There is also an ambiguous \"group\" variable which breaks the users into, for example, two different races, ethnicities, or gender groupings.\n",
    "\n",
    "![screenshot of an excerpt of the data in tabular form](screen-7.jpg)\n",
    "\n",
    "#### Basic Model\n",
    "\n",
    "- In the next step, a simple machine learning model is trained to approve or deny credit card applications based on the dataset. The results show a shocking disparity between Group A and Group B. While Group A and Group B each make up roughly half of the applicants, Group A only accounts for 20.99% of approvals, while Group B does for 79.01%. The model seems to give an unfair advantage to members in Group B, with respect to the fairness principle of demographic parity. \n",
    "\n",
    "![results of machine learning model](screen-8.jpg)\n",
    "\n",
    "- Moreover, while the accuracy of the model's performance for both groups is largely similiar, Group B also has a slight unfair advantage in terms of equal accuracy (94.56% vs 95.02%). \n",
    "\n",
    "![screenshot of confusion matrix Group A](screen-9.jpg)\n",
    "![screenshot of confusion matrix Group B](screen-10.jpg)\n",
    "\n",
    "- As shown by the confusion matrices above, in relation to the equal opportunity principle, Group B has a significant unfair advantage over group A, with a true positive rate of 98.03% compared to 77.23%. This means that if you are in Group A, you are less likely to be approved even if you meet all the criteria for approval. \n",
    "\n",
    "![visualisation of decision tree for ML model](screen-11.jpg)\n",
    "\n",
    "- A visualisation of the model's decision tree shows the process behind each approval decision. It illustrates the disadvantages members of Group A face when being assessed by the model (Group <= 0.5 is equivalent to Group A): Members of Group A are only approved if their income is greater than 88440.5. Members of Group B are approved as soon as their income is greater than 71909.5. This means that the model's decision relies on group membership for applicants whose income falls within the range of 71909.5 to 88440.5. Members of both groups are automatically denied if their income is lower than or equal to 71909.5. \n",
    "\n",
    "#### Group Unaware Model\n",
    "![results of new, group unaware model](screen-12.jpg)\n",
    "\n",
    "\n",
    "- Next, a group unaware model is trained by eliminating the group variable from the training data. This should stop the model from relying on group membership for decision-making. We can see from the results that the equal accuracy and equal opportunity criteria are now putting Group A at an advantage, with an accuracy of 93.61% and a true positive rate of 93.24%, compared to 91.72% and 86.21% for Group B, respectively. The difference in equal opportunity remains lower than in the group aware model.\n",
    "- However, we can still observe that only 31.7% of approvals belong to Group A despite using the group unaware model. While this is an improvement from the first model, the lack of demographic parity indicates that the fairness issue at hand goes deeper than the presence of a group attribute. Instead, it is likely due to historical bias and income may be a proxy variable for group membership. This means that members of Group A (most likely) have systematically lower income than members of Group B and are therefore less likely to be approved.\n",
    "\n",
    "#### Group Threshold Model\n",
    "\n",
    "- A third model is trained which aims to achieve demographic parity by implementing so-called group thresholds. This means that the model will be more lenient towards members of Group A, approving their applications even though the model has less confidence they meet the criteria. At the same time, members of Group B are evaluated more harshly and face increased denials. Such an approach is akin to the \"Algorithmic Affirmative Action\" approach I mentioned previously. It aims to compensate for historical bias in the data by actively making it easier for the historically disadvantaged group to be approved. For example, women may appear less financially stable than men due to gender pay gaps and employment pauses such as maternity leave. To compensate for this in a machine learning model, the threshold for female applicants to be approved would need to be lower than for male applicants. \n",
    "\n",
    "![results of third model tuned towards demographic parity](screen-13.jpg)\n",
    "\n",
    "- Looking at the results, we can observe that demographic parity is nearly achieved, with almost equal numbers of Group A and B individuals being approved. However, while being almost consistent across both groups, the accuracy of the group threshold model is significantly lower than that of the other models. This indicates that the model approves more individuals that would be considered \"bad applicants\" according to the original criteria than the other models. While exhibiting fairness both in terms of demographic parity and equal accuracy, the model puts members of Group A at a significant opportunity advantage with a true positive rate of 100% compared to 63.64%. This means that many applicants in Group B who should have been approved according to the original criteria were in fact denied. Both the increased approval of \"bad\" applicants in Group A and the increased denial of \"good\" applicants in Group B are intentional in line with the group threshold approach. Ultimately, whether this model is more fair than the others is a moral question and dependends on your personal attitude towards affirmative action and equality in the analogue world. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264c12a",
   "metadata": {
    "id": "1264c12a"
   },
   "source": [
    "# Task 5: AI and Explainability\n",
    "\n",
    "In this section we will explore the reasons behind decisions that AI makes. While this is really hard to know, there are some approaches developed to know which features in your data (e.g. median_income in the housing dataset we used before) played a more important role than others in determining how your machine learning model performs. One of the many approaches for assessing feature importance is **permutation importance**.\n",
    "\n",
    "The idea behind permutation importance is simple. Features are what you might consider the columns in a tabulated dataset, such as that might be found in a spreadsheet.\n",
    "- The idea of permutation importance is that a feature is important if the performance of your AI program gets messed up by **shuffling** or **permuting** the order of values in that feature column for the entries in your test data.\n",
    "- The more your AI performance gets messed up in response to the shuffling, the more likely the feature was important for the AI model.\n",
    "\n",
    "To make this idea more concrete, read through the page at the [Tutorial on Permutation Importance](https://www.kaggle.com/code/dansbecker/permutation-importance) at Kaggle. The page describes an example to \"predict a person's height when they become 20 years old, using data that is available at age 10\".\n",
    "\n",
    "The page invites you to work with code to calculate the permutation importance of features for an example in football to predict \"whether a soccer/football team will have the \"Man of the Game\" winner based on the team's statistics\". Scroll down to the end of the page to the section \"Your Turn\" where you will find a link to an exercise to try it yourself to calculate the importance of features in a Taxi Fare Prediction dataset.\n",
    "\n",
    "#### Task 1-a: Carry out the exercise, taking screenshots of the exercise as you make progress. Using screen shots and text in your notebook, answer the following question:\n",
    "1. How many features are in this dataset?\n",
    "2. Were the results of doing the exercise contrary to intuition? If yes, why? If no, why not?\n",
    "3. Discuss your results with your peer group.\n",
    "4. Include your screenshots, text, and discyssions in a markdown cell.\n",
    "\n",
    "#### Task 1-b: Reflecting on Permutation Importance.\n",
    "\n",
    "- Do you think the permutation importance is a reasonable measure of feature importance?\n",
    "- Can you think of any examples where this would have issues?\n",
    "- Discuss these questions in your notebook - describe your example, if you have any, and discuss the issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f45294",
   "metadata": {},
   "source": [
    "## Task 5: Explainability Answers\n",
    "\n",
    "### 1-a\n",
    "\n",
    "![list of features in the dataset](screen-14.jpg)\n",
    "- The dataset contains five features influencing taxi fare predictions: Pickup Latitude, Pickup Longitude, Dropoff Latitude, Dropoff Longitude and Passenger Count. \n",
    "\n",
    "![list of permutation importance results](screen-15.jpg)\n",
    "\n",
    "- It surprised me that latitude features were of more importance for the predictions than longitude features. This suggests that taxi trips in the dataset's city typically cover longer distances in terms of north-south (latitude) than east-west (longitude), which was something that I did not intuitively consider. The near zero permutation importance for passenger count confirmed my intuition because in most cities, taxis do not charge higher fares if the number of passengers is higher.\n",
    "\n",
    "![list of permutation importance results including new features](screen-16.jpg)\n",
    "\n",
    "- Next, the dataset introduced new features to measure the latitudinal and longitudinal distances the trips covered. This enabled the separation of the impact of being in specific areas of the city from the influence of the overall distance traveled. Determining the permutation importance of the newly introduced features highlights that the distance of a trip is the most important feature for predictions, which aligns with my intuition. Latitudinal distances were slightly more important than longitudinal distances, which was expected given the first results.\n",
    "\n",
    "\n",
    "### 1-b\n",
    "\n",
    "- Generally, permutation importance seems like a very reasonable measure of feature importance to me. Both the process of permutation the results at which the measure arrives appear conclusive, allowing me to correctly identify the features that have the most impact on the model's output. \n",
    "\n",
    "\n",
    "- A limitation I can think of is that the method only focusses on individual features and does not take relationships between features into account. For example, if the features of a dataset are highly correlated with each other, permutation importance  may not capture certain aspects of feature importance accurately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44978d66",
   "metadata": {
    "id": "44978d66"
   },
   "source": [
    "# Task 6: Further Activities for Broader Discussion\n",
    "\n",
    "Apart from the [**Jigsaw Toxic Comment Classification Challenge**](https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge) another challenge you might explore is the [**Inclusive Images Challenge**](https://www.kaggle.com/c/inclusive-images-challenge). Read at least one of the following.\n",
    "\n",
    "- The [announcement of the Inclusive Images Challenge made by Google AI](https://ai.googleblog.com/2018/09/introducing-inclusive-images-competition.html). Explore the [Open Images Dataset V7](https://storage.googleapis.com/openimages/web/index.html) - this is where the Inclusive Images Challenge dataset comes from.\n",
    "- Article summarising [the Inclusive Image Challenge at NeurIPS 2018 conference](https://link.springer.com/chapter/10.1007/978-3-030-29135-8_6)\n",
    "- Explore the [recent controversy](https://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias) about bias in relation to [PULSE](https://paperswithcode.com/method/pulse) which, among other things, sharpens blurry images.\n",
    "- Given your exploration in the sections above, what problems might you foresee with [these tasks attempted with the Jigsaw dataset on toxicity](https://link.springer.com/chapter/10.1007/978-981-33-4367-2_81)?\n",
    "\n",
    "There are many concepts (e.g. model cards and datasheets) omitted in discussion above about AI and Ethics. To acquire a foundational knowledge of transparency, accessibility and fairness:\n",
    "\n",
    "- You are welcome to carry out the rest of the [Kaggle course on Intro to AI Ethics](https://www.kaggle.com/learn/intro-to-ai-ethics) to see some ideas from the Kaggle community.\n",
    "- You are welcome to carry out the rest of the [Kaggle tutorial on explainability]( https://www.kaggle.com/learn/machine-learning-explainability) but these are a bit more technical in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca51345e",
   "metadata": {
    "id": "ca51345e"
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this lab, you explored a number of areas that pose challenges with regard to AI and ethics: bias, fairness and explainability. This, and other topics in reposible AI development, is currently at the forefront of the AI landscape.\n",
    "\n",
    "The discussions coming up in the lectures on applications of AI (to be presented by guest lecturers in the weeks to come) will undoubtedly intersect with these concerns. In preparation, you might think, in advance, about **what distinctive questions about ethics might arise in AI applications in law, language, finance, archives, generative AI and beyond**.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15009a35",
   "metadata": {
    "id": "15009a35"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
