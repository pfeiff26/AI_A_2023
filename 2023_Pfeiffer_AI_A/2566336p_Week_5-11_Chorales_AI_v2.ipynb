{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bd112e",
   "metadata": {
    "id": "59bd112e"
   },
   "source": [
    "# Generating Chorales With RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1FsHBlnNBaI",
   "metadata": {
    "id": "m1FsHBlnNBaI"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Step into the captivating realm of automated music composition, as I will walk you through the code of a machine learning model that aims to compose music in the style of Bach chorales. Johann Sebastian Bach was one of the most influential composers of all times, inspiring generations of artists to come with his masterful use of harmonisation techniques such as counterpoint. Bach chorales, specifically, are harmonisations of hymn melodies which contain four parts, meaning that four notes are written to be played at the same time. Today, with the help of machine learning, you too can become a musical genius like Bach. \n",
    "\n",
    "Imitating the styles of renowned artists has been a key interest of computational creativity research (a branch of artificial intelligence). The purpose of these studies is to assess whether machine-generated art has reached human-like quality. Within the context of music, Bach chorales generation is one of the most widely used case studies, because J.S. Bach wrote over 400 chorale pieces that all follow the same compositional rules - characteristics that are ideal for machine learning models. The bigger and more homogenous the collection of examples, the easier it is for a machine learning model to understand the complex underlying patterns. Imagine someone showed you different songs by the same artist and asked you what aspects of the music you would describe as their unique \"signature sound\". It would be much harder to give a useful response if you only heard two songs instead of 400. Similarly, it would be very difficult to summarise what the music has in common, if the artist ventured out into lot of different styles instead of sticking to the same format. \n",
    "\n",
    "[Source: DeepBach: a Steerable Model for Bach Chorales Generation](https://arxiv.org/pdf/1612.01010.pdf)\n",
    "\n",
    "\n",
    "![musical notes being converted into numbers](notes-numbers.jpg)\n",
    "Author's own work.\n",
    "\n",
    "The dataset used in this exercise contains the sheet music of 382 J.S. Bach chorales. For the music to be machine-readable, it has to be converted into numbers. This is achieved by representing each note as its index on the piano, with number 0 indicating the absence of a note. The chorales stored in the dataset are broken down into 1/16th time units, each unit containing four numbers which indicate the four notes being played on this count.\n",
    "\n",
    "[Dataset Source](https://github.com/ageron/handson-ml2/blob/master/15_processing_sequences_using_rnns_and_cnns.ipynb)\n",
    "\n",
    "The dataset is being fed into a \"neural network\" - an advanced machine learning model which aims to simulate the structure of the human brain. It consists of different layers of neurons (the equivalent of a brain cell) which communicate with other neurons in the layer behind them in an effort to learn patterns from the data they receive. Each neuron (also called node) in the network is like a sensor that picks up one specific pattern and sends its insights to the next, processing increasingly complex information which each subsequent layer. In this case, the neural network should learn to recognise the relationships between notes in the chorales and through that analyse the compositional style of J.S. Bach. The goal is for the neural network to be able to predict the notes which J.S. Bach would have chosen to continue a given melody. If successful, it can be used to produce novel compositions that sound very musical and are indistinguishable from real Bach chorales. \n",
    "\n",
    "Although what is considered musical and what is not is a highly subjective experience, there is some sort of common understanding for what melodies are aesthetically pleasing. The Oxford dictionary defines musical as \"having a pleasant sound; melodious or tuneful.\" Have you ever heard a child improvise on their instrument for the first time? While they might be generating notes on it, it can be quite a painful to sit through. Now compare that to a solo by a professional Jazz musician. Or listen to the [new Dua Lipa Single](https://www.youtube.com/watch?v=suAR1PYFNYA) side-by-side with [Stockhausen's Gesang der Jünglinge](https://www.youtube.com/watch?v=nffOJXcJCDg). Which one do you perceive as more melodious and pleasant-sounding? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc81fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/suAR1PYFNYA?si=yJlSXf-IoSEvkWBd\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" ></iframe>\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nffOJXcJCDg?si=snzl5sORWvuYFBoS\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" ></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080f88c",
   "metadata": {
    "id": "8080f88c"
   },
   "source": [
    "# Getting the Data\n",
    "\n",
    "The first code snippet downloads the chorale dataset from [GitHub](https://github.com), a cloud platform that is used to store and share files similiar to Dropbox but with special features for developers (e.g. collaboration features, version history). The dataset is stored in a compressed format (similiar to a .zip file) to save storage space, so the code also needs to extract the raw data to be able to work with it. [Dataset Source](https://github.com/ageron/handson-ml2/blob/master/datasets/jsb_chorales/README.md)\n",
    "\n",
    "Moreover, the code imports a python library called TensorFlow which can be thought of as a collection of pre-made machine learning tools. Importing this library allows you to use advanced machine learning models without having to program them from scratch. [TensorFlow](https://www.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d3ad52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "61d3ad52",
    "outputId": "ee305432-f586-4da7-e1b3-0b2b66066580"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.utils.get_file(\n",
    "    \"jsb_chorales.tgz\",\n",
    "    \"https://github.com/ageron/data/raw/main/jsb_chorales.tgz\",\n",
    "    cache_dir=\".\",\n",
    "    extract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tASNZuZRZo6A",
   "metadata": {
    "id": "tASNZuZRZo6A"
   },
   "source": [
    "### Training, Validation & Test Data\n",
    "\n",
    "For the model to be able to learn from the chorales dataset, it must be divided into separate training, validation and test sets, that all support either the training or the evaluation process.\n",
    "\n",
    "- The training dataset is used to teach the model the distinctive musical patterns and styles of J.S. Bach.\n",
    "\n",
    "\n",
    "- The validation dataset would then be employed to \"broaden the horizon\" of the model, ensuring that it can also make accurate predictions on chorales that were not included in the training set. This dataset validates that the model does not \"overfit\" the data inputed during training stages - meaning that it does not merely memorize the training material but can abstract from it.\n",
    "\n",
    "\n",
    "- The test set serves as a final check to see how well the model generalises to new, unseen examples. The model is shown the remaining chorales from the collection and evaluated on how accurately it can predict sequences of notes. This gives us an indication of how well the model is expected to perform in real-world scenarios. \n",
    "\n",
    "\n",
    "The validation and training dataset are used for preliminary assessments of the model's performance, based on which it is improved. The test produces an accuracy score that is representative for the model in its final form. You can think of this as a summative assessment carried out at the end of a university course, whereas the validation and training exercises are more akin to informal feedback or formative assessments that seek to track and refine the models progress. \n",
    "\n",
    "In this case, the three subsets were already pre-selected and the code only had to separate them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06abc3de",
   "metadata": {
    "id": "06abc3de"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "jsb_chorales_dir = Path(\"datasets/jsb_chorales\")\n",
    "train_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\n",
    "valid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\n",
    "test_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZYRxgMj3ah4y",
   "metadata": {
    "id": "ZYRxgMj3ah4y"
   },
   "source": [
    "The next code snippet loads the data from the three subsets inside this notebook. Previously, they were being stored in separate files but now they can be accessed directly by calling the variables train_chorales, valid_chorales and test_chorales. You can think of this as copypasting the contents from separate excel files into this document to ensure they are immediately accessible and can be edited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de8b24",
   "metadata": {
    "id": "23de8b24"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_chorales(filepaths):\n",
    "    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n",
    "\n",
    "train_chorales = load_chorales(train_files)\n",
    "valid_chorales = load_chorales(valid_files)\n",
    "test_chorales = load_chorales(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebca341",
   "metadata": {
    "id": "eebca341"
   },
   "source": [
    "# Preparing the Data\n",
    "\n",
    "This code snippet checks that the data has loaded correctly. This is achieved by extracting all individual notes within the dataset and calculating statistics about the notes, including the total number of unique notes, as well as the highest and lowest notes. The highest and lowest notes are then checked against those anticipated to ensure they are the same as expected and consequently one can assume that the dataset has loaded correctly. Since the model bases all its predictions on patterns within the training data, the output of a model is always only as good as what goes into it. This is referred to as the garbage in, garbage out concept of machine learning. It highlights that it is of uttermost importance to ensure the quality of the training data and therefore, it is important to double-check that it has loaded correctly and completely.\n",
    "\n",
    "The notes cover a range from 36 (representing C1, C in octave 1) to 81 (representing A5, A in octave 5), with the additional inclusion of 0 to denote silence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b8890",
   "metadata": {
    "id": "953b8890"
   },
   "outputs": [],
   "source": [
    "notes = set()\n",
    "for chorales in (train_chorales, valid_chorales, test_chorales):\n",
    "    for chorale in chorales:\n",
    "        for chord in chorale:\n",
    "            notes |= set(chord)\n",
    "\n",
    "n_notes = len(notes)\n",
    "min_note = min(notes - {0}) #0 denotes no notes being played\n",
    "max_note = max(notes)\n",
    "\n",
    "assert min_note == 36\n",
    "assert max_note == 81"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4f254",
   "metadata": {
    "id": "05a4f254"
   },
   "source": [
    "### Code for Synthesiser\n",
    "\n",
    "The following cell is code for a synthesiser to play MIDI. Not part of machine learning code to generate Bach, but useful for listening to the results and samples used for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc04ca",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "a0bc04ca",
    "outputId": "69e0e5f6-0afe-4735-b5c8-6d1f891080c0"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "\n",
    "def notes_to_frequencies(notes):\n",
    "    # Frequency doubles when you go up one octave; there are 12 semi-tones\n",
    "    # per octave; Note A on octave 4 is 440 Hz, and it is note number 69.\n",
    "    return 2 ** ((np.array(notes) - 69) / 12) * 440\n",
    "\n",
    "def frequencies_to_samples(frequencies, tempo, sample_rate):\n",
    "    note_duration = 60 / tempo # the tempo is measured in beats per minutes\n",
    "    # To reduce click sound at every beat, we round the frequencies to try to\n",
    "    # get the samples close to zero at the end of each note.\n",
    "    frequencies = (note_duration * frequencies).round() / note_duration\n",
    "    n_samples = int(note_duration * sample_rate)\n",
    "    time = np.linspace(0, note_duration, n_samples)\n",
    "    sine_waves = np.sin(2 * np.pi * frequencies.reshape(-1, 1) * time)\n",
    "    # Removing all notes with frequencies ≤ 9 Hz (includes note 0 = silence)\n",
    "    sine_waves *= (frequencies > 9.).reshape(-1, 1)\n",
    "    return sine_waves.reshape(-1)\n",
    "\n",
    "def chords_to_samples(chords, tempo, sample_rate):\n",
    "    freqs = notes_to_frequencies(chords)\n",
    "    freqs = np.r_[freqs, freqs[-1:]] # make last note a bit longer\n",
    "    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n",
    "                     for melody in freqs.T], axis=0)\n",
    "    n_fade_out_samples = sample_rate * 60 // tempo # fade out last note\n",
    "    fade_out = np.linspace(1., 0., n_fade_out_samples)**2\n",
    "    merged[-n_fade_out_samples:] *= fade_out\n",
    "    return merged\n",
    "\n",
    "def play_chords(chords, tempo=160, amplitude=0.1, sample_rate=44100, filepath=None):\n",
    "    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n",
    "    if filepath:\n",
    "        from scipy.io import wavfile\n",
    "        samples = (2**15 * samples).astype(np.int16)\n",
    "        wavfile.write(filepath, sample_rate, samples)\n",
    "        return display(Audio(filepath))\n",
    "    else:\n",
    "        return display(Audio(samples, rate=sample_rate))\n",
    "\n",
    "## testing the synthesiser\n",
    "for index in range(3):\n",
    "    play_chords(train_chorales[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JpGFNTVikPhr",
   "metadata": {
    "id": "JpGFNTVikPhr"
   },
   "source": [
    "## Converting chords into arpeggios\n",
    "\n",
    "To generate new chorales, the goal is to train a model that uses the preciding chords to predict subsequent chords. However, if the model tries to predict all four notes of each chord at the same time, the risk is higher that result will be non-musical. Bach chorales are harmonisations of existing hymn melodies, which implies that the connections between individual notes within a voice are more significant than dependencies at the chord level. To shift the focus onto individual notes, the developer of the model decided to convert the chords into arpeggios (see image 1). This means that the four notes of each chord are turned into sequences of notes and each note is predicted individually.\n",
    "\n",
    "![chord vs. arpeggio demonstration](chord-vs-arpeggio.jpg)\n",
    "\n",
    "[Image 1: classicalguitar.org](https://www.classicalguitar.org/classical-guitar-technique/)\n",
    "\n",
    "[Reproduced under CC BY-NC-ND License](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n",
    "\n",
    "The chorales are converted into in a storage format called windows which can be understood by the previously imported \"TensorFlow\" framework. In this case, windows are subsets of chorales each containing 32 chords in the form of 128 sequential notes. By utilising this technique, the model is trained by being fed \"bite-sized\" melodic fragments rather than whole chorales at once.\n",
    "\n",
    "### Sources:\n",
    "\n",
    "[Source 1: Aurélien Geron](https://github.com/ageron/handson-ml2/blob/master/15_processing_sequences_using_rnns_and_cnns.ipynb)\n",
    "\n",
    "[Source 2: TensorFlow Documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c315b49",
   "metadata": {
    "id": "8c315b49"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_target(batch):\n",
    "    X = batch[:, :-1]\n",
    "    Y = batch[:, 1:] # predict next note in each arpegio, at each step\n",
    "    return X, Y\n",
    "\n",
    "def preprocess(window):\n",
    "    window = tf.where(window == 0, window, window - min_note + 1) # shift values\n",
    "    return tf.reshape(window, [-1]) # convert to arpegio\n",
    "\n",
    "def bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n",
    "                 window_size=32, window_shift=16, cache=True):\n",
    "    def batch_window(window):\n",
    "        return window.batch(window_size + 1)\n",
    "\n",
    "    def to_windows(chorale):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n",
    "        return dataset.flat_map(batch_window)\n",
    "\n",
    "    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n",
    "    dataset = dataset.flat_map(to_windows).map(preprocess)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(create_target)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eXv1MFybs-Pb",
   "metadata": {
    "id": "eXv1MFybs-Pb"
   },
   "source": [
    "Newly converted into sequential form, the dataset needs to be split into training, validation and test subsets again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b547f8",
   "metadata": {
    "id": "49b547f8"
   },
   "outputs": [],
   "source": [
    "train_set = bach_dataset(train_chorales, shuffle_buffer_size=1000)\n",
    "valid_set = bach_dataset(valid_chorales)\n",
    "test_set = bach_dataset(test_chorales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa66a40",
   "metadata": {
    "id": "4aa66a40"
   },
   "source": [
    "# Building the Model\n",
    "\n",
    "The following code defines the architecture of the neural network.\n",
    "\n",
    "- The first layer is a so-called embedding layer. It is used to represent discrete categorical variables (in this case musical notes) as continuous vectors. It turns each note into a specific point in a five-dimensional space. The position of each note in this space is determined by these five numbers, and notes that are similar or share certain musical characteristics will be closer to each other in this space. Essentially, one can envision these vectors as coordinates for a map of all musical notes and each melody a route for a journey throughout this map. By having five numbers to capture attributes in, the computer can develop a more nuanced understanding of the relationships between notes compared to having a single number represent it. [Source: Keras Documentation](https://keras.io/api/layers/core_layers/embedding/)\n",
    "\n",
    "\n",
    "- This is followed by four hidden \"Conv1D\" layers. A Conv1D layer scans through the input sequence, identifies patterns using filters, and passes this learned information to the next layers of the neural network. In this context, these layers act like musical filters that look for specific patterns in the sequence of notes. Imagine you are reading the sheet music for the chorales, and with each filter, you focus on different aspects of the notes. For example, the first layer might look at pairs of consecutive notes, searching for simple patterns. Each layer analyses more complex and intricate sequences than the previous one and passes its insights on to the next. [Source: Keras Documentation](https://keras.io/api/layers/convolution_layers/convolution1d/)\n",
    "\n",
    "\n",
    "- After the convolutional layers, there is an LSTM layer. LSTM stands for Long Short-Term Memory and, as the name suggests is like the memory of the computer. It helps the model understand the context and dependencies between notes over longer sequences. This is important to capture the musical flow and structure of the chorales because temporal elements such as repetition play a big role in our perception of music. [Source: Music Generation using an LSTM](https://arxiv.org/abs/2203.12105)\n",
    "\n",
    "\n",
    "- The output layer gives out a likelihood for every note in the corpus (There are 46 unique notes and a \"0\" for silence). The note which receives the highest likelihood score is the note that the network thinks J.S. Bach himself would have used at this point in the composition, with respect to previous notes. In other words, this is the note that is stastically most likely to appear at this position. [Source: Aurélien Geron](https://github.com/ageron/handson-ml2/blob/master/15_processing_sequences_using_rnns_and_cnns.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b35ac9",
   "metadata": {
    "id": "d5b35ac9"
   },
   "outputs": [],
   "source": [
    "n_embedding_dims = 5\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_notes, output_dim=n_embedding_dims,\n",
    "                           input_shape=[None]),\n",
    "    tf.keras.layers.Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_notes, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c181e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "330c181e",
    "outputId": "94d7efa5-57bf-4aaf-d74b-030ff034e2f2"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18d5028",
   "metadata": {
    "id": "d18d5028"
   },
   "source": [
    "# Training the Model\n",
    "\n",
    "This code snippet contains the code for training the model using the training and validation sets.\n",
    "\n",
    "Here is a brief overview over the training process.\n",
    "\n",
    "![Visualisation of ML training pipeline](training-process.png)\n",
    "Source: Author's own work.\n",
    "\n",
    "- At first, the predictions of the model are going to be very random and non-musical. To be able to accurately predict the next note in a sequence, the model needs to learn which of the patterns it picks up on are the most important indicators for which note comes next. This is determined through internal parameters called weights and biases.\n",
    "\n",
    "- In each iteration of the training process, the model receives feedback on how well it did. If its prediction was close to the actual next note in a Bach chorale, it gets positive feedback. If it's wrong, it learns from the mistake. A so-called loss function is the model's measure of how wrong its prediction was, i.e. how much the predicted note diverged from the real one.  \n",
    "\n",
    "- Now, just like a musician adjusting their performance based on feedback, the model tweaks its parameters, to minimise the loss function. An Algorithm called optimiser adjusts the model's weights and biases in ways that make the loss value a little smaller with each iteration. Through this process, the model becomes incrementally better at suggesting the right most Bach-like notes in the sequence.\n",
    "\n",
    "- During the validation process, the model performs on a set of Bach chorales it has not seen during training. This is like checking how well the model can now compose new chorales in Bach's style, not just repeat what it has memorised.\n",
    "\n",
    "- The training and validation process is repeated 20 times. It goes through the entire dataset with each iteration referred to as \"epochs\".\n",
    "\n",
    "The resulting model can predict the next note in a sequence with a 93.26% accuracy for the training chorales and 81.57% for the validitation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682f58a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7682f58a",
    "outputId": "bd9ba9bf-dda9-4132-a916-61a43307f7dc"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=20, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e9dbdf",
   "metadata": {
    "id": "29e9dbdf"
   },
   "source": [
    "# Saving and Evaluating Your Model\n",
    "\n",
    "Here it becomes relevant again that we set aside a number of chorales for evaluation purposes. The model's performance is now evaluated on the test set, containing chorales the model has not seen before. It achieves an accuracy of 81.33% in identifying the correct note following an input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea7b1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cea7b1d",
    "outputId": "81b91578-6fc6-4e26-fd9a-4bf3b04d4374"
   },
   "outputs": [],
   "source": [
    "model.save(\"my_bach_model\", save_format=\"tf\")\n",
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f1e7c1",
   "metadata": {
    "id": "13f1e7c1"
   },
   "source": [
    "# Generating Chorales\n",
    "\n",
    "The final blocks of code are putting the newly trained and evaluated model to use.\n",
    "\n",
    "The code provides the model with a set of two initial chords, which it will transform into arpeggios. The model will then predict the succeeding note and continue this process iteratively. Finally, the generated notes will be organized into groups of four to reconstruct chords, forming the resulting chorale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a628834",
   "metadata": {
    "id": "5a628834"
   },
   "outputs": [],
   "source": [
    "def generate_chorale_v2(model, seed_chords, length, temperature=1):\n",
    "    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n",
    "    arpegio = tf.reshape(arpegio, [1, -1])\n",
    "    for chord in range(length):\n",
    "        for note in range(4):\n",
    "            next_note_probas = model.predict(arpegio)[0, -1:]\n",
    "            rescaled_logits = tf.math.log(next_note_probas) / temperature\n",
    "            next_note = tf.random.categorical(rescaled_logits, num_samples=1)\n",
    "            arpegio = tf.concat([arpegio, next_note], axis=1)\n",
    "    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
    "    return tf.reshape(arpegio, shape=[-1, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KWSGuLJiiRqd",
   "metadata": {
    "id": "KWSGuLJiiRqd"
   },
   "source": [
    "## Seed Chords\n",
    "\n",
    "This short code snippet is extracting the first eight chords from a random test chorale. (Technically, it only picks out two chords, which repeat four times each)\n",
    "\n",
    "The synthesiser programmed earlier makes the selected chords audible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e17c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "id": "026e17c0",
    "outputId": "c7a739ea-75e5-4d73-fed7-404ef2c5b809"
   },
   "outputs": [],
   "source": [
    "seed_chords = test_chorales[2][:8]\n",
    "play_chords(seed_chords, amplitude=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qyhz97Bzlkcc",
   "metadata": {
    "id": "qyhz97Bzlkcc"
   },
   "source": [
    "## Chorale Generation Parameters\n",
    "\n",
    "To generate a new chorale, the model requires the initial chords and for the user to set a desried length of the chorale. In this case, the model will generate 56 chords in addition to the 8 initial chords.\n",
    "\n",
    "Aiming to minimize risks, the model consistently opts for the note with the highest score, often leading to monotonous melodies. Running the model multiple times results in the generation of identical melodies.\n",
    "\n",
    "To address this issue, the developer introduced an additional measure called temperature. The temperature determines how \"daring\" the model acts. Instead of always choosing the note with the highest score it will randomly choose between all suggested notes according to their probability. If the temperature is set higher, the model will choose a note that is more unlikely to come next in the sequence. Think of a this as allowing the model to \"go wild\" and free itself from the rules it has learnt. \n",
    "\n",
    "![Animal Muppet GIF playing the drums](animal.gif)\n",
    "[Image Source: Tenor](https://tenor.com/bjEhG.gif)\n",
    "\n",
    "[Source: Aurélien Geron](https://github.com/ageron/handson-ml2/blob/master/15_processing_sequences_using_rnns_and_cnns.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9f623",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f0b9f623",
    "outputId": "6f0ac740-0641-49da-bdd1-a5341a148638"
   },
   "outputs": [],
   "source": [
    "new_chorale_v2_cold = generate_chorale_v2(model, seed_chords, 56, temperature=0.8)\n",
    "play_chords(new_chorale_v2_cold, filepath=\"bach_cold.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26bfdfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d26bfdfc",
    "outputId": "757de904-011c-42b4-a0c7-d83549254488"
   },
   "outputs": [],
   "source": [
    "new_chorale_v2_medium = generate_chorale_v2(model, seed_chords, 56, temperature=1.0)\n",
    "play_chords(new_chorale_v2_medium, filepath=\"bach_medium.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48734911",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "48734911",
    "outputId": "4139c9dc-aa87-4939-9bff-c7b4d0a8f42e"
   },
   "outputs": [],
   "source": [
    "new_chorale_v2_hot = generate_chorale_v2(model, seed_chords, 56, temperature=1.5)\n",
    "play_chords(new_chorale_v2_hot, filepath=\"bach_hot.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0_6uNUvbphr0",
   "metadata": {
    "id": "0_6uNUvbphr0"
   },
   "source": [
    "# Reflection\n",
    "\n",
    "### Evaluating the results\n",
    "\n",
    "- Unlike other forms of generative AI, music generation systems are usually evaluated by humans listening to the machine compositions ([Carnovalini and Rodà (2020)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7861321/) ). Therefore, I decided to judge the performance of the model by listening to the generated examples. The chorale generated with the \"cold\" temperature setting sounds the most musical but, as pointed out before, it often holds notes significantly longer than what the listener would expect. Consequently, while the harmonisations sound Bach-like and aesthetically pleasing, the rhythmic composition of the chorale sounds uninteresting and artificial. While increasing the temperature brings more diversity into the compositions, it makes them sound less aesthetically pleasing and stylistically accurate. Overall, while the statistical evaluation resulted in an accuracy of over 80%, the results still lack a deeper sense of musicality that can be hard to capture with numbers.\n",
    "\n",
    "### Own Application\n",
    "\n",
    "- I would like to use the machine learning techniques featured in the code to apply them to a different musical context, such as pop music. The required dataset would need to be similarly structured to the existing one. To achieve a similiar four-voice format, it could contain one main vocal melody, one backing vocal harmony, one high-pitched and one low pitched instrumental part.  \n",
    "\n",
    "- I am interested in exploring if it would perform better with pop music since it is often considered to contain simpler melodies and harmonies. At the same time, the compositional rules are largely less strict and it will be hard to find a comparably homogenous training corpus. I am also just curious to see what kind of songs the model would come up with and if they sound musical at all.\n",
    "\n",
    "- To feed the pop songs into the computer they would first need to be manually transcribed to sheet music and then converted into numerical form. Most modern productions include more than four voices so one would have to decide which instrumental parts to use. The preprocessing workflow would be very similiar to the one outlined in this exercise.\n",
    "\n",
    "- This model and similiar AIs generate music based on existing composition. While J.S. Bach's music is in the public domain, datasets containing more recent compostions that are protected by copyright, there is a concern about the originality and ownership of the generated music. If any AI-generated compositions are published, there is a risk of unintentional plagiarism. Also composers are usually not compensated when their music is used as training data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
